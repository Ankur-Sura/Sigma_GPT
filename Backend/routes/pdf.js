// =============================================================================
//                     PDF ROUTES - RAG Document Upload & Query
// =============================================================================
/**
 * ğŸ“š WHAT IS THIS FILE?
 * ---------------------
 * Handles PDF upload and question-answering using RAG.
 * 
 * ğŸ”— ENDPOINTS:
 * -------------
 * POST /api/upload-pdf  â†’ Upload PDF, extract text, create embeddings
 * POST /api/pdf-query   â†’ Ask questions about uploaded PDF
 * 
 * ğŸ“Œ RAG PIPELINE:
 * ----------------
 *     1. User uploads PDF
 *            â†“
 *     2. Backend forwards to Python AI service
 *            â†“
 *     3. AI service: Extract text â†’ Chunk â†’ Embed â†’ Store in Qdrant
 *            â†“
 *     4. Returns pdf_id for future queries
 *            â†“
 *     5. User asks question with pdf_id
 *            â†“
 *     6. AI service: Embed question â†’ Search Qdrant â†’ Get relevant chunks
 *            â†“
 *     7. LLM generates answer using chunks as context
 * 
 * ğŸ“Œ INTERVIEW TIP:
 * -----------------
 * "My RAG system uses vector embeddings for semantic search. When a user
 *  uploads a PDF, I chunk it, embed each chunk with OpenAI, and store in
 *  Qdrant. Questions are answered by finding similar chunks and using
 *  them as context for the LLM."
 */

import express from "express";
// ğŸ“– Express for routing

import multer from "multer";
// ğŸ“– Multer: Middleware for handling file uploads
// ğŸ“Œ Why multer?
// - Parses multipart/form-data (file uploads)
// - Stores files in memory or disk
// - Provides file info (name, size, mimetype)

import Thread from "../models/Thread.js";
// ğŸ“– For persisting Q&A to chat history

const router = express.Router();

const upload = multer({storage: multer.memoryStorage()});
// ğŸ“– memoryStorage(): Keep file in RAM, don't save to disk
// ğŸ“Œ Why memory?
// - We're forwarding to AI service immediately
// - No need to persist file on Backend
// - Faster than disk I/O

const AI_SERVICE_URL = process.env.AI_SERVICE_URL || "http://localhost:8000";
// ğŸ“– Python AI service URL (FastAPI)


// =============================================================================
//                         PDF UPLOAD ENDPOINT
// =============================================================================
/**
 * ğŸ“– POST /api/upload-pdf
 * -----------------------
 * Uploads a PDF file and forwards it to the AI service for processing.
 * 
 * Request: multipart/form-data with "pdf" field
 * Response: { pdf_id, filename, status, chunk_count }
 * 
 * ğŸ“Œ WHAT HAPPENS IN AI SERVICE:
 * 1. Extract text from PDF (PyPDFLoader)
 * 2. Split into chunks (RecursiveCharacterTextSplitter)
 * 3. Create embeddings (OpenAI text-embedding-3-small)
 * 4. Store in Qdrant vector database
 * 5. Return pdf_id for future queries
 */

router.post("/upload-pdf", upload.single("pdf"), async (req, res) => {
// ğŸ“– upload.single("pdf"): Multer middleware
// - Expects file in form field named "pdf"
// - Makes file available as req.file
    const file = req.file;
    // ğŸ“– req.file: The uploaded file (from multer middleware)
    // Contains: buffer, originalname, mimetype, size

    // ğŸ“Œ Get threadId for persistence
    const threadId = req.body?.threadId;

    if(!file) {
        return res.status(400).json({error: "No file uploaded"});
    }

    try {
        // =================================================================
        // Forward PDF to Python AI Service
        // =================================================================
        const formData = new FormData();
        // ğŸ“– FormData: Web API for multipart/form-data
        // ğŸ“Œ Why FormData? AI service expects multipart file upload

        formData.append("file", new Blob([file.buffer], {type: file.mimetype}), file.originalname);
        // ğŸ“– Blob: Binary Large Object
        // - file.buffer: Raw file bytes from multer
        // - file.mimetype: "application/pdf"
        // - file.originalname: "document.pdf"
        // 
        // ğŸ“Œ WHY BLOB?
        // Convert buffer to Blob so fetch can send it as multipart

        // =================================================================
        // ğŸ”— CONNECTION: BACKEND â†’ AI SERVICE (Python FastAPI)
        // =================================================================
        // This is where Node.js Backend calls the Python AI Service!
        // 
        // FLOW:
        // Frontend (React) â†’ [You are here: Backend] â†’ AI Service (Python)
        //                                                    â†“
        //                                            RAG Pipeline:
        //                                            Extract â†’ Chunk â†’ Embed â†’ Qdrant
        //
        // ğŸ“Œ AI endpoint: AI/rag_service.py â†’ upload_pdf()
        // =================================================================
        const response = await fetch(`${AI_SERVICE_URL}/pdf/upload`, {
            method: "POST",
            body: formData
            // ğŸ“Œ Note: No "Content-Type" header!
            // fetch automatically sets multipart/form-data with boundary
        });

        const data = await response.json();
        console.log("ğŸ“„ PDF Upload response:", {pdf_id: data.pdf_id, filename: data.filename, status: data.status});

        if(!response.ok) {
            return res.status(response.status).json({error: data.detail || "PDF upload failed"});
        }

        // =================================================================
        // ğŸ†• Persist PDF Upload to MongoDB
        // =================================================================
        /**
         * ğŸ“– WHY PERSIST PDF UPLOAD?
         * --------------------------
         * So when user reopens the chat thread, they see:
         * - What PDF they uploaded
         * - That they can ask questions about it
         * 
         * Without this, PDF upload notifications disappear on refresh!
         * 
         * ğŸ“Œ INTERVIEW TIP:
         * "I persist all AI tool interactions to MongoDB, including file
         *  uploads, so the complete conversation history is preserved."
         */
        if(threadId) {
            try {
                let thread = await Thread.findOne({threadId});
                const filename = data.filename || file.originalname;
                
                if(!thread) {
                    thread = new Thread({
                        threadId,
                        title: `PDF: ${filename}`,
                        messages: []
                    });
                }
                
                // Add assistant message for upload notification
                const uploadMessage = `ğŸ“„ **${filename}** uploaded.\nAsk about specific pages or content.`;
                thread.messages.push({role: "assistant", content: uploadMessage});
                
                thread.updatedAt = new Date();
                await thread.save();
                console.log("âœ… PDF upload saved to thread:", threadId);
            } catch(saveErr) {
                console.log("âš ï¸ Failed to save PDF upload to thread:", saveErr);
                // Don't fail the request
            }
        }

        return res.json(data);
        // ğŸ“– Returns: { pdf_id, filename, status, chunk_count }
        // Frontend stores pdf_id for future queries
    } catch(err) {
        console.log(err);
        return res.status(500).json({error: "Failed to forward PDF to AI service"});
    }
});


// =============================================================================
//                         PDF QUERY ENDPOINT
// =============================================================================
/**
 * ğŸ“– POST /api/pdf-query
 * ----------------------
 * Asks a question about an uploaded PDF.
 * 
 * Body: { pdf_id, question, threadId }
 * Response: { answer, context, sources }
 * 
 * ğŸ“Œ RAG QUERY FLOW:
 * 1. Embed the question (same model as chunks)
 * 2. Search Qdrant for similar chunks (vector similarity)
 * 3. Take top-k most similar chunks
 * 4. Give chunks to LLM as context
 * 5. LLM generates answer grounded in the document
 * 
 * ğŸ“Œ INTERVIEW TIP:
 * "When querying, I embed the question using the same model as the documents.
 *  Then I use cosine similarity to find the most relevant chunks. This ensures
 *  the LLM answers based on actual document content, not hallucinations."
 */

router.post("/pdf-query", async (req, res) => {
    const {pdf_id, question, threadId} = req.body || {};
    // ğŸ“– pdf_id: Identifier returned from upload
    // ğŸ“– question: User's question about the PDF
    // ğŸ“– threadId: For persisting to chat history
    
    console.log("ğŸ“ PDF Query received:", {pdf_id, question, threadId});
    
    if(!pdf_id || !question) {
        console.log("âŒ Missing pdf_id or question");
        return res.status(400).json({error: "missing pdf_id or question"});
    }

    try {
        // =================================================================
        // ğŸ”— CONNECTION: BACKEND â†’ AI SERVICE (Python FastAPI)
        // =================================================================
        // This is where Node.js Backend calls the Python AI Service!
        // 
        // FLOW:
        // Frontend (React) â†’ [You are here: Backend] â†’ AI Service (Python)
        //                                                    â†“
        //                                            RAG Query Pipeline:
        //                                            Embed Question â†’ Search Qdrant
        //                                            â†’ Get Context â†’ LLM Answer
        //
        // ğŸ“Œ AI endpoint: AI/rag_service.py â†’ query_pdf()
        // =================================================================
        /**
         * ğŸ†• Now passing thread_id for conversation memory!
         * -------------------------------------------------
         * This allows follow-up questions about the PDF:
         * 
         * User: "What skills are in this resume?"
         * AI:   "Python, JavaScript, React..."
         * User: "Tell me more about Python"  â† This now works!
         * 
         * The AI service saves Q&A to MongoDB and loads history
         * for subsequent questions in the same thread.
         */
        console.log("ğŸ”„ Forwarding to AI service with thread_id...");
        const response = await fetch(`${AI_SERVICE_URL}/pdf/query`, {
            method: "POST",
            headers: {
                "Content-Type": "application/json"
            },
            body: JSON.stringify({
                pdf_id, 
                question,
                thread_id: threadId || "default"  // ğŸ†• Pass thread for memory!
            })
        });
        // ğŸ“– AI service does the RAG magic:
        // - Embeds question
        // - Searches Qdrant
        // - ğŸ†• Loads conversation history for this thread
        // - Calls LLM with context + history
        // - ğŸ†• Saves Q&A for future follow-ups
        // - Returns grounded answer

        const data = await response.json();

        if(!response.ok) {
            return res.status(response.status).json({error: data.detail || "PDF query failed"});
        }

        // =================================================================
        // Persist Q&A to Thread History
        // =================================================================
        // ğŸ“Œ WHY PERSIST?
        // So user sees PDF Q&A in chat history after refresh
        
        if(threadId) {
            try {
                let thread = await Thread.findOne({threadId});
                if(!thread) {
                    thread = new Thread({
                        threadId,
                        title: question,
                        messages: [{role: "user", content: question}]
                    });
                } else {
                    thread.messages.push({role: "user", content: question});
                }
                thread.messages.push({role: "assistant", content: data.answer || "No answer"});
                thread.updatedAt = new Date();
                await thread.save();
            } catch (persistErr) {
                console.log("Failed to persist PDF Q&A:", persistErr);
                // ğŸ“Œ Don't fail the request just because persistence failed
            }
        }

        return res.json(data);
    } catch(err) {
        console.log(err);
        return res.status(500).json({error: "Failed to query AI service"});
    }
});


// =============================================================================
//                              EXPORT ROUTER
// =============================================================================

export default router;


// =============================================================================
//                         SUMMARY FOR INTERVIEWS
// =============================================================================
/**
 * ğŸ“Œ RAG PIPELINE SUMMARY:
 * 
 * UPLOAD PHASE:
 *     PDF File â†’ PyPDFLoader â†’ Text â†’ Chunking â†’ Embeddings â†’ Qdrant
 * 
 * QUERY PHASE:
 *     Question â†’ Embed â†’ Search Qdrant â†’ Top-K Chunks â†’ LLM â†’ Answer
 * 
 * ğŸ“Œ KEY TECHNOLOGIES:
 * - Multer: File upload handling
 * - FormData/Blob: Binary data transfer
 * - OpenAI Embeddings: text-embedding-3-small
 * - Qdrant: Vector similarity search
 * - LangChain: Document loading and chunking
 * 
 * ğŸ“Œ WHY RAG?
 * - LLMs have training cutoff date
 * - LLMs can hallucinate facts
 * - RAG grounds answers in real documents
 * - User can verify from source
 */
